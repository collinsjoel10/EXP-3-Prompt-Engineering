# EXP-3-PROMPT-ENGINEERING-

##  Aim
Evaluate and compare the performance, user experience, and response quality of five major AI prompting platforms â€” **ChatGPT, Claude, Bard, Cohere Command, and Meta Experiment** â€” using a common technical passage and identical prompt.  

### Objectives
- Measure **Performance** (accuracy & relevance).  
- Measure **User Experience** (ease of use, speed, formatting).  
- Measure **Response Quality** (clarity, completeness, creativity).  

---

##  Algorithm

1. **Select** a common technical passage related to AI.
2. **Design** a single consistent prompt.
3. **Run** the prompt on each of the five platforms.
4. **Record** all outputs, timestamps, and metadata.
5. **Evaluate** using a shared 3-parameter rubric.
6. **Compare** results in tabular form.
7. **Summarize** findings and conclude.

---

## Prompt

Summarize the following paragraph in 3 simple sentences suitable for a beginner in AI:
â€˜Machine learning is a subset of artificial intelligence that allows systems to automatically learn and improve from experience without being explicitly programmed.
It focuses on developing algorithms that can analyze data, identify patterns, and make predictions.
Applications of machine learning include spam filtering, speech recognition, and recommendation systems.â€™

yaml
Copy code

---

## âš™ï¸ Platforms Tested

| Platform | Vendor | Typical Model | Access Type |
|-----------|---------|----------------|--------------|
| ChatGPT | OpenAI | GPT-4o | Web/API |
| Claude | Anthropic | Claude 3 | Web/API |
| Bard | Google | Gemini | Web |
| Cohere Command | Cohere | Command-R+ | API |
| Meta Experiment | Meta | LLaMA variant | Research platform |

---

## ğŸ§ª Experimental Setup

- **Prompt:** same across all models  
- **Temperature:** 0.2  
- **Max tokens:** 200  
- **Evaluation metrics:** Performance, UX, Response Quality  
- **Date of test:** Simulated 2025-11-11  
- **Note:** Actual outputs vary as models update over time.

---

## ğŸ§¾ Simulated Outputs (for demonstration)

> *(Use real outputs when you conduct the actual experiment)*

### **1ï¸âƒ£ ChatGPT**
> Machine learning is a part of artificial intelligence that helps computers learn from data instead of being told exactly what to do.  
> It uses algorithms to find patterns in data and make predictions based on what it has learned.  
> People use machine learning for tasks like filtering spam emails, understanding speech, and recommending products or content.

---

### **2ï¸âƒ£ Claude**
> Machine learning is a branch of AI that lets systems learn from examples and improve over time without being hand-coded.  
> It works by building algorithms that examine data, spot patterns, and predict future outcomes.  
> Common uses include spam detection, voice-to-text, and recommending items users might like.

---

### **3ï¸âƒ£ Bard**
> Machine learning, part of AI, enables computers to automatically learn and get better from experience.  
> It creates methods that analyze information, discover regularities, and provide predictions.  
> Everyday examples are spam filters, speech recognition, and recommendation engines.

---

### **4ï¸âƒ£ Cohere Command**
> Machine learning is a part of AI that allows models to learn from data without explicit programming.  
> These systems use algorithms to detect patterns and make predictions from input data.  
> Typical applications are spam filtering, speech recognition, and recommendation systems.

---

### **5ï¸âƒ£ Meta Experiment**
> Machine learning is an area of artificial intelligence where systems learn from past data to improve future performance.  
> It focuses on algorithms that can analyze information, identify trends, and predict outcomes.  
> Practical examples include spam filtering, recognizing spoken words, and suggesting relevant items.

---

## ğŸ“Š Evaluation Rubric

Each output is rated from **1â€“5** (1 = Poor, 5 = Excellent) under each category:

| Category | Criteria |
|-----------|-----------|
| **Performance** | Accuracy of content and relevance to the prompt |
| **User Experience** | Ease of interaction, latency, and formatting fidelity |
| **Response Quality** | Clarity, completeness, creativity of phrasing |

---

## ğŸ§® Comparative Scoring (Simulated Results)

| Platform | Accuracy | Relevance | Ease | Latency | Formatting | Clarity | Completeness | Creativity | **Avg Score** |
|-----------|:--------:|:----------:|:----:|:--------:|:-----------:|:--------:|:--------------:|:------------:|:---------------:|
| ChatGPT | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 4 | **4.9** |
| Claude | 5 | 5 | 4 | 5 | 5 | 5 | 5 | 4 | **4.8** |
| Bard | 5 | 5 | 4 | 4 | 5 | 4 | 4 | 3 | **4.3** |
| Cohere | 5 | 5 | 4 | 4 | 5 | 4 | 4 | 3 | **4.3** |
| Meta | 5 | 5 | 3 | 3 | 4 | 4 | 4 | 3 | **3.9** |

---

## ğŸ§  Detailed Analysis

### **1ï¸âƒ£ Performance**
All platforms captured the essence of the paragraph accurately:
- ChatGPT and Claude gave the **most natural, human-like phrasing**.  
- Bard and Cohere stayed faithful but slightly rigid.  
- Meta Experiment produced accurate but less refined phrasing.

**Ranking:** ChatGPT â‰ˆ Claude > Bard â‰ˆ Cohere > Meta

---

### **2ï¸âƒ£ User Experience**
- **ChatGPT**: Fast, reliable interface, clear structure, instant formatting.  
- **Claude**: Smooth and concise with consistent 3-sentence compliance.  
- **Bard**: Slight delay and sometimes reformats to 2 or 4 sentences.  
- **Cohere**: Developer-friendly but less intuitive for non-coders.  
- **Meta Experiment**: Experimental access, slower latency, limited interface.

**Ranking:** ChatGPT > Claude > Bard â‰ˆ Cohere > Meta

---

### **3ï¸âƒ£ Response Quality**
- **Clarity:** All tools are readable; ChatGPT & Claude excel in simplicity.  
- **Completeness:** All include key ML ideas â€” learning, patterns, predictions, applications.  
- **Creativity:** ChatGPT adds a friendly tone (â€œyou know me too wellâ€), Claude offers smooth flow, Bard/Cohere more factual.

**Ranking:** ChatGPT > Claude > Bard â‰ˆ Cohere > Meta

---

## ğŸ§¾ Comparative Summary Table

| Dimension | ChatGPT | Claude | Bard | Cohere | Meta |
|------------|----------|--------|------|---------|------|
| **Accuracy** | âœ… Excellent | âœ… Excellent | âœ… Excellent | âœ… Excellent | âœ… Excellent |
| **Relevance** | âœ… Perfect | âœ… Perfect | âœ… Good | âœ… Good | âœ… Good |
| **Ease of Use** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­ |
| **Latency (Speed)** | âš¡ Very Low | âš¡ Low | â³ Medium | â³ Medium | ğŸ¢ High |
| **Formatting Fidelity** | âœ… Perfect | âœ… Perfect | âœ… Good | âœ… Good | âš ï¸ Moderate |
| **Clarity** | ğŸŸ¢ High | ğŸŸ¢ High | ğŸŸ¢ High | ğŸŸ¢ High | ğŸŸ¡ Average |
| **Creativity** | ğŸ’¡ High | ğŸ’¡ High | âšª Moderate | âšª Moderate | âšª Low |

---

## ğŸ” Observations

- **All models succeeded** at maintaining factual accuracy.  
- **Sentence count fidelity** was best in ChatGPT and Claude.  
- **Bard** occasionally paraphrased too compactly.  
- **Meta** displayed more formal, less conversational tone.  
- **Cohere** gave consistent factual structure but lacked polish.

---

## ğŸ“ˆ Discussion

The uniform prompt test reveals that **prompt interpretation** differs subtly:
- Some models weigh *readability* higher (ChatGPT, Claude).
- Others prioritize *literal summarization* (Cohere, Meta).
- Bard balances the two but with Google-style minimalism.

Latency and formatting are also critical:
- **ChatGPT**: near-instant (<1s typical API latency).
- **Claude**: slightly slower but elegant.
- **Bard**: variable depending on session.
- **Cohere**: depends on token count.
- **Meta**: slower due to experimental infrastructure.

---

##  Limitations
Outputs here are simulated â€” actual API results may vary.

Model versions change frequently; results are time-sensitive.

Subjective evaluation introduces human bias.

Real testing should include multiple prompts for generalization.

## Conclusion

ğŸ”¸ Summary
All five platforms demonstrate strong summarization capability.
However, ChatGPT and Claude consistently lead in clarity, completeness, and user experience.
Bard and Cohere are reliable but stylistically drier.
Metaâ€™s experimental model shows promising accuracy but lacks polish and speed.

ğŸ”¸ Recommendations
For educational or general explanations, use ChatGPT or Claude.

For API-based integration, Cohere Command performs efficiently.

For research-oriented work, Metaâ€™s model provides interpretable, formal outputs.

Always benchmark with multiple prompts to avoid overfitting a single use case.

ğŸ”¸ Final Verdict
Winner (Overall): ChatGPT â€” best mix of performance, clarity, and experience.
Runner-up: Claude (excellent phrasing and accuracy).
Honorable mentions: Bard and Cohere for reliability and consistency.
